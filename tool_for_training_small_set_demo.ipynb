{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tool_for_training_small_set_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1n7QhgLSaJVvFvDuN8Cui7VQsPbISYejt",
      "authorship_tag": "ABX9TyNdqY1h1AouxD3oRbQ+LPR+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/96jonesa/StyleGan2-Colab-Demo/blob/master/tool_for_training_small_set_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeOfcFCLy6F9",
        "colab_type": "text"
      },
      "source": [
        "# You're probably looking for this:\n",
        "\n",
        "IF YOU WOULD LIKE TO SEE A COMPARISON OF RESULTS FROM TRAINING ON VARIOUS\n",
        "DATASETS UNDER VARIOUS CONFIGURATIONS, CHECK OUT THIS NOTEBOOK:\n",
        "\n",
        "https://colab.research.google.com/drive/1uwPlY-4P_6fJ59SFRtgZLebVGgwGrUQu?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDuLAzjizDWc",
        "colab_type": "text"
      },
      "source": [
        "# What is this?\n",
        "\n",
        "This is a simple demo of use of an open-source PyTorch implementation of StyleGAN2\n",
        "\n",
        "https://github.com/lucidrains/stylegan2-pytorch\n",
        "\n",
        "setup for training using Colab's free GPU resources and Google Drive. Citations at bottom.\n",
        "\n",
        "The GitHub repo I made for this project is available at:\n",
        "\n",
        "https://github.com/96jonesa/StyleGan2-Colab-Demo\n",
        "\n",
        "# Where do the files go?\n",
        " \n",
        "Training results and models are saved to the local runtime's 'results' and 'models' directories, or to your Google Drive in subdirectories (of the same names) of a parent directory named 'StyleGan2_small_set_demo'.\n",
        "\n",
        "Local runtime files can be accessed by clicking on the folder icon found on the toolbar to the left.\n",
        "\n",
        "# Using your Google Drive:\n",
        "\n",
        "IF YOU CHOOSE TO USE YOUR GOOGLE DRIVE for training, then you will be prompted in Code Cell 2 of this notebook to authorize access. You must click a link, copy a code, and paste it into the input box below Code Cell 2. Hit enter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA1sJxmhzXlh",
        "colab_type": "text"
      },
      "source": [
        "# HOW TO USE:\n",
        "\n",
        "0. Login to Google (Drive)\n",
        "\n",
        "1. Click 'Copy to Drive' above to make a runnable copy of this notebook.\n",
        "2. Run this cell (click the play button in top left of cell) to connect to a runtime instance.\n",
        "3. Navigate to 'Runtime > Change Runtime Type > Hardware Accelerator' and select GPU.\n",
        "4. Modify the variables found in the cell below to select behavior of demo.\n",
        "5. Run all cells ('Runtime > Run All').\n",
        "6. IF USING YOUR GOOGLE DRIVE, FOLLOW INSTRUCTIONS FOUND IN ABOVE CELL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8lZLs-H3-_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 'celeba', 'afhq', 'metafaces', 'cifar10', 'afhq_dog', 'afhq_cat', 'afhq_wild',\n",
        "# 'cifar10_airplane', 'cifar10_automobile', 'cifar10_bird', 'cifar10_cat', 'cifar10_deer',\n",
        "# 'cifar10_dog', 'cifar10_frog', 'cifar10_horse', 'cifar10_ship', 'cifar10_truck'\n",
        "USE_DATASET = 'celeba'\n",
        "\n",
        "TRAINING_FROM_SCRATCH = False # set True if training from scratch, False if training for last checkpoint\n",
        "MODEL_NAME = 'default'\n",
        "MODEL_NUM_TRAIN_STEPS = 3000\n",
        "LOW_NETWORK_CAPACITY = False # set True to use significantly lower network capacity\n",
        "USE_GOOGLE_DRIVE_FOR_TRAINING = False\n",
        "\n",
        "# 'none', 'first', 'every'\n",
        "USE_ATTENTION_LAYERS = 'none' # which layers do you want attention applied to?\n",
        "\n",
        "MODEL_AUGMENTATION_PROBABILITY = 0.0\n",
        "MODEL_LEARNING_RATE = 2e-4\n",
        "MODEL_IMAGE_SIZE = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DBbOtcUlE2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mounts your Google Drive so files can be saved to it. Note that this also allows\n",
        "# files to be read from it, so only authorize this if you are comfortable doing so\n",
        "# and/or using a disposable Google Drive account.\n",
        "\n",
        "if USE_GOOGLE_DRIVE_FOR_TRAINING:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    !mkdir -p \"/content/drive/My Drive/StyleGan2_small_set_demo\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBVovyA8pNZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prints information about the GPU allocated by Colab.\n",
        "# Possible models of GPU are K80, T4, and P100. K80 is relatively slow.\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EKhglHWE6bW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_NETWORK_CAPACITY = 16\n",
        "if LOW_NETWORK_CAPACITY:\n",
        "    MODEL_NETWORK_CAPACITY = 4\n",
        "\n",
        "MODEL_ATTENTION_LAYERS = []\n",
        "if USE_ATTENTION_LAYERS == 'first':\n",
        "    MODEL_ATTENTION_LAYERS = \"[1]\"\n",
        "elif USE_ATTENTION_LAYERS == 'every':\n",
        "    MODEL_ATTENTION_LAYERS = \"[1,2,3,4,5,6]\"\n",
        "\n",
        "MODEL_NAME = USE_DATASET + '_' + MODEL_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ShcNlPdbOnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Installs the architecture from:\n",
        "# https://github.com/lucidrains/stylegan2-pytorch\n",
        "\n",
        "!pip install stylegan2_pytorch==0.17.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv3pbXk1f7f7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilities for downloading publicly shared Google Drive files (from my Google Drive).\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = 'https://docs.google.com/uc?export=download'\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, 'wb') as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUNC4_8K7LWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install linformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVkgs8NSgHEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloads and unzips the selected dataset from my Google Drive.\n",
        "\n",
        "import zipfile\n",
        "\n",
        "if USE_DATASET == 'celeba':\n",
        "\n",
        "    file_id = '1tF3yRlv5VZx0hgZ5RsNdOiAlSrrIPJNq'\n",
        "    destination = 'celeba.zip'\n",
        "    download_file_from_google_drive(file_id, destination)\n",
        "    zip_ref = zipfile.ZipFile('celeba.zip', 'r')\n",
        "    zip_ref.extractall('data/celeba')\n",
        "    zip_ref.close()\n",
        "\n",
        "elif USE_DATASET in ['afhq', 'afhq_dog', 'afhq_cat', 'afhq_wild']:\n",
        "\n",
        "    file_id = '1PFZbFOQzGhXUmF6TxAzboEP7fOn8zyHa'\n",
        "    destination = 'afhq.zip'\n",
        "    download_file_from_google_drive(file_id, destination)\n",
        "    zip_ref = zipfile.ZipFile('afhq.zip', 'r')\n",
        "    zip_ref.extractall('data/afhq')\n",
        "    zip_ref.close()\n",
        "\n",
        "elif USE_DATASET == 'metfaces':\n",
        "    file_id = '1r7XMa8gNZqwqEsgf9t7MFWP7BAgy74jm'\n",
        "    destination = 'metfaces.zip'\n",
        "    download_file_from_google_drive(file_id, destination)\n",
        "    zip_ref = zipfile.ZipFile('metfaces.zip', 'r')\n",
        "    zip_ref.extractall('data/metfaces')\n",
        "    zip_ref.close()\n",
        "\n",
        "elif USE_DATASET == 'brecahad':\n",
        "    file_id = '1lnZd9ujC3FecVc9dmjOm_XUX3ei3B_69'\n",
        "    destination = 'brecahad.zip'\n",
        "    download_file_from_google_drive(file_id, destination)\n",
        "    zip_ref = zipfile.ZipFile('brecahad.zip', 'r')\n",
        "    zip_ref.extractall('data/brecahad')\n",
        "    zip_ref.close()\n",
        "\n",
        "elif USE_DATASET in ['cifar10', 'cifar10_airplane', 'cifar10_automobile', 'cifar10_bird', 'cifar10_cat', 'cifar10_deer', 'cifar10_dog', 'cifar10_frog', 'cifar10_horse', 'cifar10_ship', 'cifar10_truck']:\n",
        "    file_id = '1T_maRdj_fgXLychhORRivbyOflhzISGV'\n",
        "    destination = 'cifar10.zip'\n",
        "    download_file_from_google_drive(file_id, destination)\n",
        "    zip_ref = zipfile.ZipFile('cifar10.zip', 'r')\n",
        "    zip_ref.extractall('data/cifar10')\n",
        "    zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqsmQTyLan3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chooses the appropriate subdirectory of dataset for training.\n",
        "\n",
        "# uses CelebA dataset by default (if chosen, or if invalid dataset name)\n",
        "MODEL_DATA_DIR = 'data/celeba/img_align_celeba'\n",
        "\n",
        "if USE_DATASET == 'metfaces':\n",
        "    MODEL_DATA_DIR = 'data/metfaces/images'\n",
        "elif USE_DATASET == 'brecahad':\n",
        "    MODEL_DATA_DIR = 'data/brecahad/images'\n",
        "elif USE_DATASET == 'afhq':\n",
        "    MODEL_DATA_DIR = 'data/afhq/afhq/train'\n",
        "elif USE_DATASET == 'cifar10':\n",
        "    MODEL_DATA_DIR = 'data/cifar10/cifar10/cifar10/train'\n",
        "elif USE_DATASET in ['afhq_dog', 'afhq_cat', 'afhq_wild']:\n",
        "    MODEL_DATA_DIR = 'data/afhq/afhq/train/' + USE_DATASET[5:]\n",
        "elif USE_DATASET in ['cifar10_airplane', 'cifar10_automobile', 'cifar10_bird', 'cifar10_cat', 'cifar10_deer', 'cifar10_dog', 'cifar10_frog', 'cifar10_horse', 'cifar10_ship', 'cifar10_truck']:\n",
        "    MODEL_DATA_DIR = 'data/cifar10/cifar10/cifar10/train/' + USE_DATASET[8:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj5WpwcZRMZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Establish directories for custom models.\n",
        "\n",
        "CUSTOM_RESULTS_DIR = './results'\n",
        "CUSTOM_MODELS_DIR = './models'\n",
        "\n",
        "if USE_GOOGLE_DRIVE_FOR_TRAINING:\n",
        "    CUSTOM_RESULTS_DIR = '\"/content/drive/My Drive/StyleGan2_small_set_demo/results\"'\n",
        "    CUSTOM_MODELS_DIR = '\"/content/drive/My Drive/StyleGan2_small_set_demo/models\"'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQUV0N8HQh1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train custom models.\n",
        "\n",
        "if TRAINING_FROM_SCRATCH:\n",
        "    !stylegan2_pytorch --data {MODEL_DATA_DIR} --name {MODEL_NAME} --new --network_capacity {MODEL_NETWORK_CAPACITY} --batch_size 4 \\\n",
        "        --gradient_accumulate_every 4 --num_train_steps {MODEL_NUM_TRAIN_STEPS} --attn_layers {MODEL_ATTENTION_LAYERS} --image_size {MODEL_IMAGE_SIZE} \\\n",
        "        --aug_prob {MODEL_AUGMENTATION_PROBABILITY} --results_dir {CUSTOM_RESULTS_DIR} --models_dir {CUSTOM_MODELS_DIR} --learning_rate {MODEL_LEARNING_RATE}\n",
        "else:\n",
        "    !stylegan2_pytorch --data {MODEL_DATA_DIR} --name {MODEL_NAME} --network_capacity {MODEL_NETWORK_CAPACITY} --batch_size 4 \\\n",
        "        --gradient_accumulate_every 4 --num_train_steps {MODEL_NUM_TRAIN_STEPS} --attn_layers {MODEL_ATTENTION_LAYERS} --image_size {MODEL_IMAGE_SIZE} \\\n",
        "        --aug_prob {MODEL_AUGMENTATION_PROBABILITY} --results_dir {CUSTOM_RESULTS_DIR} --models_dir {CUSTOM_MODELS_DIR} --learning_rate {MODEL_LEARNING_RATE}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNHw8TwH0vGY",
        "colab_type": "text"
      },
      "source": [
        "# Parameters accepted by model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7SFoPxpcgtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameter                 | default   | description\n",
        "#                           |           |\n",
        "# data                      | ./data    | directory containing data\n",
        "# results_dir               | ./results | directory for checkpoint sample images\n",
        "# models_dir                | ./models  | directory for checkpoint models (saves to and loads from here)\n",
        "# name                      | default   | name to identify model (all outputs will be saved to results_dir/name and models_dir/name)\n",
        "# new                       | False     | if True then starts from scratch, else loads from saved checkpoint model\n",
        "# load_from                 | -1        | if -1 then loads from most recent checkpoint, else loads from checkpoint number load_from\n",
        "# image_size                | 128       | size of (square) images generated and for resizing of data\n",
        "# network_capacity          | 16        | affects number of nodes per layer - decrease to train faster with lower output quality\n",
        "# transparent               | False     | if True then uses RGBA, else uses RGB\n",
        "# batch_size                | 3         | number of images per mini-batch (larger uses more GPU memory)\n",
        "# gradient_accumulate_every | 5         | number of mini-batches to process before optimizing (choice depends on batch_size)\n",
        "# num_train_steps           | 150000    | total steps of forward prop (counting starts from number of steps completed in loaded checkpoint)\n",
        "# learning_rate             | 2e-4      | learning rate\n",
        "# num_workers               | None      | if None then uses as many workers as possible from available CPU cores (for data loading)\n",
        "# save_every                | 1000      | every save_every steps, a checkpoint model and sample images are saved\n",
        "# generate                  | False     | if True then generates sample images from loaded model instead of training\n",
        "# generate_interpolation    | False     | if True then generates .gif interpolation from loaded model instead of training, else does not\n",
        "# num_image_tiles           | 8         | generated samples will be a grid of (num_image_tiles x num_image_tiles) images\n",
        "# trunc_psi                 | 0.75      | affects how far generate images can be from average image (increase for more diversity) w_new = psi * w + (1 - psi) * w_avg\n",
        "# fp16                      | False     | if True then uses fp16 half-precision to lower GPU memory usage (requires apex), else uses full-precision\n",
        "# cl_reg                    | False     | if True then uses contrastive learning on discriminator (possibly improves stability and quality), else does not\n",
        "# fq_layers                 | []        | list of layers to apply feature (intermediate representation) vector quantization to (can improve results, but not dramatically)\n",
        "# fq_dict_size              | 256       | dictionary size for feature quantization\n",
        "# attn_layers               | []        | list of layers to apply self-attention to while training (can be empty; do not use spaces; up to log2(image_size) - 1 layers)\n",
        "# no_const                  | False     | if True then 4x4 block is learned from style vector, else styles a constant learned 4x4 block through progressive upsampling\n",
        "# aug_prob                  | 0.0       | probability of applying differentiable augmentation to images fed to discriminator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgGDIUY76Rsz",
        "colab_type": "text"
      },
      "source": [
        "# CITATIONS:\n",
        "\n",
        "```\n",
        "@inproceedings{choi2020starganv2,\n",
        "  title={StarGAN v2: Diverse Image Synthesis for Multiple Domains},\n",
        "  author={Yunjey Choi and Youngjung Uh and Jaejun Yoo and Jung-Woo Ha},\n",
        "  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n",
        "  year={2020}\n",
        "}\n",
        "\n",
        "@inproceedings{liu2015faceattributes,\n",
        " title = {Deep Learning Face Attributes in the Wild},\n",
        " author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},\n",
        " booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},\n",
        " month = {December},\n",
        " year = {2015} \n",
        "}\n",
        "\n",
        "@article{Karras2019stylegan2,\n",
        "  title   = {Analyzing and Improving the Image Quality of {StyleGAN}},\n",
        "  author  = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},\n",
        "  journal = {CoRR},\n",
        "  volume  = {abs/1912.04958},\n",
        "  year    = {2019},\n",
        "}\n",
        "\n",
        "@misc{zhao2020feature,\n",
        "    title   = {Feature Quantization Improves GAN Training},\n",
        "    author  = {Yang Zhao and Chunyuan Li and Ping Yu and Jianfeng Gao and Changyou Chen},\n",
        "    year    = {2020}\n",
        "}\n",
        "\n",
        "@misc{chen2020simple,\n",
        "    title   = {A Simple Framework for Contrastive Learning of Visual Representations},\n",
        "    author  = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},\n",
        "    year    = {2020}\n",
        "}\n",
        "\n",
        "@article{,\n",
        "  title     = {Oxford 102 Flowers},\n",
        "  author    = {Nilsback, M-E. and Zisserman, A., 2008},\n",
        "  abstract  = {A 102 category dataset consisting of 102 flower categories, commonly occuring in the United Kingdom. Each class consists of 40 to 258 images. The images have large scale, pose and light variations.}\n",
        "}\n",
        "\n",
        "@article{afifi201911k,\n",
        "  title   = {11K Hands: gender recognition and biometric identification using a large dataset of hand images},\n",
        "  author  = {Afifi, Mahmoud},\n",
        "  journal = {Multimedia Tools and Applications}\n",
        "}\n",
        "\n",
        "@misc{zhang2018selfattention,\n",
        "    title   = {Self-Attention Generative Adversarial Networks},\n",
        "    author  = {Han Zhang and Ian Goodfellow and Dimitris Metaxas and Augustus Odena},\n",
        "    year    = {2018},\n",
        "    eprint  = {1805.08318},\n",
        "    archivePrefix = {arXiv}\n",
        "}\n",
        "\n",
        "@article{shen2019efficient,\n",
        "  author    = {Zhuoran Shen and\n",
        "               Mingyuan Zhang and\n",
        "               Haiyu Zhao and\n",
        "               Shuai Yi and\n",
        "               Hongsheng Li},\n",
        "  title     = {Efficient Attention: Attention with Linear Complexities},\n",
        "  journal   = {CoRR},  \n",
        "  year      = {2018},\n",
        "  url       = {http://arxiv.org/abs/1812.01243},\n",
        "}\n",
        "\n",
        "@misc{zhao2020image,\n",
        "    title  = {Image Augmentations for GAN Training},\n",
        "    author = {Zhengli Zhao and Zizhao Zhang and Ting Chen and Sameer Singh and Han Zhang},\n",
        "    year   = {2020},\n",
        "    eprint = {2006.02595},\n",
        "    archivePrefix = {arXiv}\n",
        "}\n",
        "\n",
        "@misc{karras2020training,\n",
        "    title   = {Training Generative Adversarial Networks with Limited Data},\n",
        "    author  = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},\n",
        "    year    = {2020},\n",
        "    eprint  = {2006.06676},\n",
        "    archivePrefix = {arXiv},\n",
        "    primaryClass = {cs.CV}\n",
        "}\n",
        "\n",
        "@article{article,\n",
        "author = {Krizhevsky, Alex},\n",
        "year = {2012},\n",
        "month = {05},\n",
        "pages = {},\n",
        "title = {Learning Multiple Layers of Features from Tiny Images},\n",
        "journal = {University of Toronto}\n",
        "}\n",
        "\n",
        "@misc{karras2020training,\n",
        "    title={Training Generative Adversarial Networks with Limited Data},\n",
        "    author={Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},\n",
        "    year={2020},\n",
        "    eprint={2006.06676},\n",
        "    archivePrefix={arXiv},\n",
        "    primaryClass={cs.CV}\n",
        "}\n",
        "\n",
        "@article{article,\n",
        "author = {Aksac, Alper and Demetrick, Douglas and Ozyer, Tansel},\n",
        "year = {2019},\n",
        "month = {12},\n",
        "pages = {},\n",
        "title = {BreCaHAD: a dataset for breast cancer histopathological annotation and diagnosis},\n",
        "volume = {12},\n",
        "journal = {BMC Research Notes},\n",
        "doi = {10.1186/s13104-019-4121-7}\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ]
    }
  ]
}