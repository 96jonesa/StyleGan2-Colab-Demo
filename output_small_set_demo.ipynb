{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "output_small_set_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1n7QhgLSaJVvFvDuN8Cui7VQsPbISYejt",
      "authorship_tag": "ABX9TyNyjJmPV6ZY5Omv/iwivrJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/96jonesa/StyleGan2-Colab-Demo/blob/master/output_small_set_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kroOml2bv6aP",
        "colab_type": "text"
      },
      "source": [
        "# What is this?\n",
        "\n",
        "This is a demo to walk people through the results of training StyleGAN2 on various small datasets and under various configurations. Explanations of the effects of certain critical techniques are provided. Most if not all of these models were trained (at the time of writing this) for only about 6 hours on a single P100 GPU via Colab (typically with 4 models training on separate P100s simultaneously). Typically, models of this nature would be trained up to 100x as long. These will be trained further and this demo will be updated frequently in the near future. However, the results are already decent enough for the purposes of demonstration and comparison.\n",
        "\n",
        "All of the models were trained using a notebook I wrote to allow for easy training using the free resources provided by Colab and Google Drive, linked here:\n",
        "\n",
        "https://colab.research.google.com/drive/1QWMCwSIdDb3GQp7lHqWc0RAcnI2FANBw?usp=sharing\n",
        "\n",
        "Using the PyTorch implementation of StyleGAN2 available at:\n",
        "\n",
        "https://github.com/lucidrains/stylegan2-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBnrSalUwC-B",
        "colab_type": "text"
      },
      "source": [
        "#How to use:\n",
        "\n",
        "Login to Google (Drive).\n",
        "\n",
        "Image displays are full size 8x8 grids of 128x128 images (32x32 in case of cifar10). You will need to scroll through the cell output to see each of the (typically 4) grids displayed in each output. Simply place you cursor somewhere inside the output, then scroll.\n",
        "\n",
        "You can either run all the cells then scroll through with 'Runtime > Run all'\n",
        "\n",
        "(this will cause a lot of images to print and can be tedious to scroll through)\n",
        "\n",
        "Or you can step through cell-by-cell by running each cell individually with CMD+ENTER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVuwdtUVwcWM",
        "colab_type": "text"
      },
      "source": [
        "#Descriptions of datasets (citations at end of notebook):\n",
        "\n",
        "metfaces: \"image dataset of human faces extracted from works of art\" -NVlabs\n",
        "\n",
        "celeba: \"large-scale face attributes dataset with more than 200K celebrity images\" -Lie et al.\n",
        "\n",
        "afhq_dog: \"5,000 high-quality images [of dogs] at 512Ã—512 resolution\" -Choi et al.\n",
        "\n",
        "cifar10_horse: 5000 32x32 color images of horses -Krizhevsky"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJLZThgv-iZQ",
        "colab_type": "text"
      },
      "source": [
        "# Download the sample images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4ribKtPmLVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilities for downloading publicly shared Google Drive files (from my Google Drive).\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = 'https://docs.google.com/uc?export=download'\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, 'wb') as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WK_hj5WP7yE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "from IPython.display import Image, display\n",
        "\n",
        "file_id = '1uEuLbFWRdAbaB2p3pvQOUUMj7_FJF3bN'\n",
        "destination = 'demo_samples.zip'\n",
        "download_file_from_google_drive(file_id, destination)\n",
        "zip_ref = zipfile.ZipFile('demo_samples.zip', 'r')\n",
        "zip_ref.extractall('demo_samples')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JStqPjwEwpkV",
        "colab_type": "text"
      },
      "source": [
        "# First things first\n",
        "\n",
        "First, we will look at sample images generated by the pretrained models using no differentiable augmentation and no attention. We expect the results to lack diversity (due to the lack of differentiable augmentation) and be of relatively poor quality (due to the lack of attention). In order, these 8x8 image displays are from training on the following datasets:\n",
        "\n",
        "1. cifar10_horse  (32x32 resolution, 5000 images, 97000 iterations @ 4.52 it/s on P100 GPU)\n",
        "2. afhq_dog       (128x128 resolution, 5000 images, 38000 iterations @ 1.78 it/s on P100 GPU)\n",
        "3. metfaces       (128x128 resolution, 1336 images, 27000 iterations @ 1.27 it/s on P100 GPU)\n",
        "4. celeba         (128x128 resolution, 202599 images, 36000 iterations @ 1.64 it/s on P100 GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8l6_KpDVYKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('cifar10_horse, no differentiable augmentation, no attention, standard')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_97.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('afhq_dog, no differentiable augmentation, no attention, standard')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_38.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('metfaces, no differentiable augmentation, no attention, standard')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_27.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('celeba, no differentiable augmentation, no attention, standard')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_36.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j8HbxbLw3PM",
        "colab_type": "text"
      },
      "source": [
        "# EMA generator parameters\n",
        "\n",
        "Notice the poor quality? The model also generates samples from a version of the model which use an exponential moving average of the generator parameters to compensate for the oscillatory tendencies of GANs. This improves stability and quality significantly. The result is shown below for the same models as above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOQyIGBgbjUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('cifar10_horse, no differentiable augmentation, no attention, EMA generator parameters')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_97-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('afhq_dog, no differentiable augmentation, no attention, EMA generator parameters')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_38-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('metfaces, no differentiable augmentation, no attention, EMA generator parameters')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_27-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('celeba, no differentiable augmentation, no attention, EMA generator parameters')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_36-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8RP_euQxJ2-",
        "colab_type": "text"
      },
      "source": [
        "# Mixed regularities\n",
        "\n",
        "The model also generates samples using EMA generator parameters as well as mixed regularities (using two latent space noise vectors as input to the generator, and switching from one to the other at a fixed point in the synthesis, essentially mixing the styles which would be generated by the two), which causes a decorrelation of neighboring styles and thus allows for more fine-tuned diversity (at the cost of a bit of image quality from the amount of training):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nzbcMn3Y7Es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('cifar10_horse, no differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_97-mr.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('afhq_dog, no differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_38-mr.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('metfaces, no differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_27-mr.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('celeba, no differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_36-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U83NAbbQxT3G",
        "colab_type": "text"
      },
      "source": [
        "# Early stopping\n",
        "\n",
        "By now you have likely noticed the extremely low quality of the afhq_dog model. This model has so little data to work with that it degenerates after only a few hours of training. Here are the EMA noise and mixed regularities outputs from an earlier training checkpoint of the same model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6QlLK-tdk4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('cifar10_horse early stop, no differentiable augmentation, no attention, EMA generator parameters')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_22-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('cifar10_horse early stop, no differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_22-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSmgsqMCveFx",
        "colab_type": "text"
      },
      "source": [
        "# Differentiable augmentation\n",
        "\n",
        "By now you have likely noticed the extreme lack of diversity in the afhq_dog and metfaces models, as well as a more subtle lack of diversity in the cifar10_horses and celeba models. This is due to the small amount of data available in the former two datasets, and a reasonable yet still lacking amount of data in the latter two datasets. A recent innovation in data augmentation which uses differentiable augmentations of the data has led to models obtaining high quality results with up to 70x less data. The following show the results of training the above models while applying this augmentation with probability 0.2 to at each iteration (the EMA generator parameters results are shown, without mixed regularities). In order, these were trained on:\n",
        "\n",
        "1. cifar10_horse  (32x32 resolution, 5000 images, 97000 iterations @ 4.51 it/s on P100 GPU)\n",
        "2. afhq_dog       (128x128 resolution, 5000 images, 38000 iterations @ 1.56 it/s on P100 GPU)\n",
        "3. metfaces       (128x128 resolution, 1336 images, 27000 iterations @ 1.05 it/s on P100 GPU)\n",
        "4. celeba         (128x128 resolution, 202599 images, 36000 iterations @ 1.64 it/s on P100 GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wxng40HeZ3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('cifar10_horse, 0.2 differentiable augmentation, no attention, EMA generator parameters')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_02_attn_none_97-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('afhq_dog, 0.2 differentiable augmentation, no attention, EMA generator parameters')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_02_attn_none_38-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('metfaces, 0.2 differentiable augmentation, no attention, EMA generator parameters')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_02_attn_none_27-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('celeba, 0.2 differentiable augmentation, no attention, EMA generator parameters')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_02_attn_none_36-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHzZXNVNxaB1",
        "colab_type": "text"
      },
      "source": [
        "# Cool, but let's see that with mixed regularities\n",
        "\n",
        "The same augmented data models produced the following sample images using mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQfuI0nip1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The same augmented data models produced the following sample images using mixed regularities:\n",
        "\n",
        "print('cifar10_horse, 0.2 differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_02_attn_none_97-mr.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('afhq_dog, 0.2 differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_02_attn_none_38-mr.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('metfaces, 0.2 differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_02_attn_none_27-mr.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('celeba, 0.2 differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_02_attn_none_36-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBufbo4Ox570",
        "colab_type": "text"
      },
      "source": [
        "# Coming soon:\n",
        "\n",
        "Results from training using attention on every layer, with and without augmentation.\n",
        "\n",
        "Results from training with various other differentiable augmentation probabilities (0.1 and 0.3).\n",
        "\n",
        "Results from training for more iterations under these configurations.\n",
        "\n",
        "Results from training with larger image sizes (on the high-resolution datasets).\n",
        "\n",
        "Results from training on additional interesting small datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0GhmmYg4dyM",
        "colab_type": "text"
      },
      "source": [
        "# Citations:\n",
        "\n",
        "```\n",
        "@inproceedings{choi2020starganv2,\n",
        "  title={StarGAN v2: Diverse Image Synthesis for Multiple Domains},\n",
        "  author={Yunjey Choi and Youngjung Uh and Jaejun Yoo and Jung-Woo Ha},\n",
        "  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n",
        "  year={2020}\n",
        "}\n",
        "\n",
        "@inproceedings{liu2015faceattributes,\n",
        " title = {Deep Learning Face Attributes in the Wild},\n",
        " author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},\n",
        " booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},\n",
        " month = {December},\n",
        " year = {2015} \n",
        "}\n",
        "\n",
        "@article{Karras2019stylegan2,\n",
        "  title   = {Analyzing and Improving the Image Quality of {StyleGAN}},\n",
        "  author  = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},\n",
        "  journal = {CoRR},\n",
        "  volume  = {abs/1912.04958},\n",
        "  year    = {2019},\n",
        "}\n",
        "\n",
        "@misc{zhao2020feature,\n",
        "    title   = {Feature Quantization Improves GAN Training},\n",
        "    author  = {Yang Zhao and Chunyuan Li and Ping Yu and Jianfeng Gao and Changyou Chen},\n",
        "    year    = {2020}\n",
        "}\n",
        "\n",
        "@misc{chen2020simple,\n",
        "    title   = {A Simple Framework for Contrastive Learning of Visual Representations},\n",
        "    author  = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},\n",
        "    year    = {2020}\n",
        "}\n",
        "\n",
        "@article{,\n",
        "  title     = {Oxford 102 Flowers},\n",
        "  author    = {Nilsback, M-E. and Zisserman, A., 2008},\n",
        "  abstract  = {A 102 category dataset consisting of 102 flower categories, commonly occuring in the United Kingdom. Each class consists of 40 to 258 images. The images have large scale, pose and light variations.}\n",
        "}\n",
        "\n",
        "@article{afifi201911k,\n",
        "  title   = {11K Hands: gender recognition and biometric identification using a large dataset of hand images},\n",
        "  author  = {Afifi, Mahmoud},\n",
        "  journal = {Multimedia Tools and Applications}\n",
        "}\n",
        "\n",
        "@misc{zhang2018selfattention,\n",
        "    title   = {Self-Attention Generative Adversarial Networks},\n",
        "    author  = {Han Zhang and Ian Goodfellow and Dimitris Metaxas and Augustus Odena},\n",
        "    year    = {2018},\n",
        "    eprint  = {1805.08318},\n",
        "    archivePrefix = {arXiv}\n",
        "}\n",
        "\n",
        "@article{shen2019efficient,\n",
        "  author    = {Zhuoran Shen and\n",
        "               Mingyuan Zhang and\n",
        "               Haiyu Zhao and\n",
        "               Shuai Yi and\n",
        "               Hongsheng Li},\n",
        "  title     = {Efficient Attention: Attention with Linear Complexities},\n",
        "  journal   = {CoRR},  \n",
        "  year      = {2018},\n",
        "  url       = {http://arxiv.org/abs/1812.01243},\n",
        "}\n",
        "\n",
        "@misc{zhao2020image,\n",
        "    title  = {Image Augmentations for GAN Training},\n",
        "    author = {Zhengli Zhao and Zizhao Zhang and Ting Chen and Sameer Singh and Han Zhang},\n",
        "    year   = {2020},\n",
        "    eprint = {2006.02595},\n",
        "    archivePrefix = {arXiv}\n",
        "}\n",
        "\n",
        "@misc{karras2020training,\n",
        "    title   = {Training Generative Adversarial Networks with Limited Data},\n",
        "    author  = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},\n",
        "    year    = {2020},\n",
        "    eprint  = {2006.06676},\n",
        "    archivePrefix = {arXiv},\n",
        "    primaryClass = {cs.CV}\n",
        "}\n",
        "\n",
        "@article{article,\n",
        "author = {Krizhevsky, Alex},\n",
        "year = {2012},\n",
        "month = {05},\n",
        "pages = {},\n",
        "title = {Learning Multiple Layers of Features from Tiny Images},\n",
        "journal = {University of Toronto}\n",
        "}\n",
        "\n",
        "@misc{karras2020training,\n",
        "    title={Training Generative Adversarial Networks with Limited Data},\n",
        "    author={Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},\n",
        "    year={2020},\n",
        "    eprint={2006.06676},\n",
        "    archivePrefix={arXiv},\n",
        "    primaryClass={cs.CV}\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ]
    }
  ]
}