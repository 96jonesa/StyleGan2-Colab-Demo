{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "output_small_set_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/96jonesa/StyleGan2-Colab-Demo/blob/master/output_small_set_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8nUSceYkGaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All of the models were trained using a notebook I wrote to allow for easy training\n",
        "# using the free resources provided by Colab and Google Drive, linked here:\n",
        "#\n",
        "# https://colab.research.google.com/drive/1QWMCwSIdDb3GQp7lHqWc0RAcnI2FANBw?usp=sharing\n",
        "#\n",
        "# Using the PyTorch implementation of StyleGAN2 available at:\n",
        "#\n",
        "# https://github.com/lucidrains/stylegan2-pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDSqjqnUkcHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How to use:\n",
        "#\n",
        "# Login to Google (Drive).\n",
        "#\n",
        "# Image displays are full size 8x8 grids of 128x128 images (32x32 in case of cifar10).\n",
        "# You will need to scroll through the cell output to see each of the (typically 4)\n",
        "# grids displayed in each output. Simply place you cursor somewhere inside the output,\n",
        "# then scroll.\n",
        "#\n",
        "# You can either run all the cells then scroll through with 'Runtime > Run all'\n",
        "#   (this will cause a lot of images to print and can be tedious to scroll through)\n",
        "#\n",
        "# Or you can step through cell-by-cell by running each cell individually with CMD+ENTER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV7tZsVD3PNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Descriptions of datasets (citations at end of notebook):\n",
        "#\n",
        "# metfaces: \"image dataset of human faces extracted from works of art\" -NVlabs\n",
        "# celeba: \"large-scale face attributes dataset with more than 200K celebrity images\" -Lie et al.\n",
        "# afhq_dog: \"5,000 high-quality images [of dogs] at 512Ã—512 resolution\" -Choi et al.\n",
        "# cifar10_horse: 5000 32x32 color images of horses -Krizhevsky"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4ribKtPmLVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilities for downloading publicly shared Google Drive files (from my Google Drive).\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = 'https://docs.google.com/uc?export=download'\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, 'wb') as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WK_hj5WP7yE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "from IPython.display import Image, display\n",
        "\n",
        "file_id = '1uEuLbFWRdAbaB2p3pvQOUUMj7_FJF3bN'\n",
        "destination = 'demo_samples.zip'\n",
        "download_file_from_google_drive(file_id, destination)\n",
        "zip_ref = zipfile.ZipFile('demo_samples.zip', 'r')\n",
        "zip_ref.extractall('demo_samples')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8l6_KpDVYKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, we will look at sample images generated by the pretrained models using no differentiable augmentation and no attention.\n",
        "# We expect the results to lack diversity (due to the lack of differentiable augmentation) and be of relatively poor quality\n",
        "# (due to the lack of attention). In order, these 8x8 image displays are from training on the following datasets:\n",
        "#\n",
        "# 1. cifar10_horse  (32x32 resolution, small dataset (5000 images), 97000 iterations @ 4.52 it/s on P100 GPU)\n",
        "# 2. afhq_dog       (128x128 resolution, small dataset (5000 images), 38000 iterations @ 1.78 it/s on P100 GPU)\n",
        "# 3. metfaces       (128x128 resolution, small dataset (1336 images), 27000 iterations @ 1.27 it/s on P100 GPU)\n",
        "# 4. celeba         (128x128 resolution, large dataset (202599 images), 36000 iterations @ 1.64 it/s on P100 GPU)\n",
        "\n",
        "print('cifar10_horse, no differentiable augmentation, no attention, standard')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_97.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('afhq_dog, no differentiable augmentation, no attention, standard')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_38.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('metfaces, no differentiable augmentation, no attention, standard')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_27.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('celeba, no differentiable augmentation, no attention, standard')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_36.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOQyIGBgbjUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Notice the poor quality? The model also generates samples from a version of the model which applies multiplicative\n",
        "# Gaussian noise to each convolutional layer in the discriminator, with the magnitude of the noise is a function of\n",
        "# the exponential moving average of the discriminator output. This improves stability and quality significantly.\n",
        "# The result is shown below for the same models as above:\n",
        "\n",
        "print('cifar10_horse, no differentiable augmentation, no attention, EMA noise')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_97-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('afhq_dog, no differentiable augmentation, no attention, EMA noise')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_38-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('metfaces, no differentiable augmentation, no attention, EMA noise')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_27-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('celeba, no differentiable augmentation, no attention, EMA noise')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_36-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nzbcMn3Y7Es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The model also generates samples using this EMA noise as well as mixed regularities (using two latent space noise vectors\n",
        "# as input to the generator, and switching from one to the other at a fixed point in the synthesis, essentially mixing\n",
        "# the styles which would be generated by the two), which causes a decorrelation of neighboring styles and thus allows\n",
        "# for more fine-tuned diversity (at the cost of a bit of image quality from the amount of training):\n",
        "\n",
        "print('cifar10_horse, no differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_97-mr.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('afhq_dog, no differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_38-mr.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('metfaces, no differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_27-mr.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('celeba, no differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_36-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6QlLK-tdk4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# By now you have likely noticed the extremely low quality of the afhq_dog model. This model has so little data to work with\n",
        "# that it degenerates after only a few hours of training. Here are the EMA noise and mixed regularities outputs from an\n",
        "# earlier training checkpoint of the same model:\n",
        "\n",
        "print('cifar10_horse early stop, no differentiable augmentation, no attention, EMA noise')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_22-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('cifar10_horse early stop, no differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_22-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wxng40HeZ3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# By now you have likely noticed the extreme lack of diversity in the afhq_dog and metfaces models, as well as a more\n",
        "# subtle lack of diversity in the cifar10_horses and celeba models. This is due to the small amount of data available in\n",
        "# the former two datasets, and a reasonable yet still lacking amount of data in the latter two datasets.\n",
        "# A recent innovation in data augmentation which uses differentiable augmentations of the data has led to models obtaining\n",
        "# high quality results with up to 70x less data. The following show the results of training the above models while applying\n",
        "# this augmentation with probability 0.2 to at each iteration (the EMA noise results are shown, without mixed regularities).\n",
        "# In order, these were trained on:\n",
        "#\n",
        "# 1. cifar10_horse  (32x32 resolution, small dataset (5000 images), 97000 iterations @ 4.51 it/s on P100 GPU)\n",
        "# 2. afhq_dog       (128x128 resolution, small dataset (5000 images), 38000 iterations @ 1.56 it/s on P100 GPU)\n",
        "# 3. metfaces       (128x128 resolution, small dataset (1336 images), 27000 iterations @ 1.05 it/s on P100 GPU)\n",
        "# 4. celeba         (128x128 resolution, large dataset (202599 images), 36000 iterations @ 1.64 it/s on P100 GPU)\n",
        "\n",
        "print('cifar10_horse, 0.2 differentiable augmentation, no attention, EMA noise')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_02_attn_none_97-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('afhq_dog, 0.2 differentiable augmentation, no attention, EMA noise')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_02_attn_none_38-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('metfaces, 0.2 differentiable augmentation, no attention, EMA noise')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_02_attn_none_27-ema.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('celeba, 0.2 differentiable augmentation, no attention, EMA noise')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_02_attn_none_36-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQfuI0nip1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The same augmented data models produced the following sample images using mixed regularities:\n",
        "\n",
        "print('cifar10_horse, 0.2 differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_02_attn_none_97-mr.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('afhq_dog, 0.2 differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_02_attn_none_38-mr.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('metfaces, 0.2 differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_02_attn_none_27-mr.jpg'))\n",
        "print()\n",
        "print()\n",
        "print('celeba, 0.2 differentiable augmentation, no attention, mixed regularities')\n",
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_02_attn_none_36-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVsv6EDok4JV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# COMING SOON:\n",
        "#\n",
        "# Results from training using attention on every layer, with and without augmentation.\n",
        "#\n",
        "# Results from training with various other differentiable augmentation probabilities (0.1 and 0.3).\n",
        "#\n",
        "# Results from training for more iterations under these configurations."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0GhmmYg4dyM",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "CITATIONS:\n",
        "\n",
        "@inproceedings{choi2020starganv2,\n",
        "  title={StarGAN v2: Diverse Image Synthesis for Multiple Domains},\n",
        "  author={Yunjey Choi and Youngjung Uh and Jaejun Yoo and Jung-Woo Ha},\n",
        "  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n",
        "  year={2020}\n",
        "}\n",
        "\n",
        "@inproceedings{liu2015faceattributes,\n",
        " title = {Deep Learning Face Attributes in the Wild},\n",
        " author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},\n",
        " booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},\n",
        " month = {December},\n",
        " year = {2015} \n",
        "}\n",
        "\n",
        "@article{Karras2019stylegan2,\n",
        "  title   = {Analyzing and Improving the Image Quality of {StyleGAN}},\n",
        "  author  = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},\n",
        "  journal = {CoRR},\n",
        "  volume  = {abs/1912.04958},\n",
        "  year    = {2019},\n",
        "}\n",
        "\n",
        "@misc{zhao2020feature,\n",
        "    title   = {Feature Quantization Improves GAN Training},\n",
        "    author  = {Yang Zhao and Chunyuan Li and Ping Yu and Jianfeng Gao and Changyou Chen},\n",
        "    year    = {2020}\n",
        "}\n",
        "\n",
        "@misc{chen2020simple,\n",
        "    title   = {A Simple Framework for Contrastive Learning of Visual Representations},\n",
        "    author  = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},\n",
        "    year    = {2020}\n",
        "}\n",
        "\n",
        "@article{,\n",
        "  title     = {Oxford 102 Flowers},\n",
        "  author    = {Nilsback, M-E. and Zisserman, A., 2008},\n",
        "  abstract  = {A 102 category dataset consisting of 102 flower categories, commonly occuring in the United Kingdom. Each class consists of 40 to 258 images. The images have large scale, pose and light variations.}\n",
        "}\n",
        "\n",
        "@article{afifi201911k,\n",
        "  title   = {11K Hands: gender recognition and biometric identification using a large dataset of hand images},\n",
        "  author  = {Afifi, Mahmoud},\n",
        "  journal = {Multimedia Tools and Applications}\n",
        "}\n",
        "\n",
        "@misc{zhang2018selfattention,\n",
        "    title   = {Self-Attention Generative Adversarial Networks},\n",
        "    author  = {Han Zhang and Ian Goodfellow and Dimitris Metaxas and Augustus Odena},\n",
        "    year    = {2018},\n",
        "    eprint  = {1805.08318},\n",
        "    archivePrefix = {arXiv}\n",
        "}\n",
        "\n",
        "@article{shen2019efficient,\n",
        "  author    = {Zhuoran Shen and\n",
        "               Mingyuan Zhang and\n",
        "               Haiyu Zhao and\n",
        "               Shuai Yi and\n",
        "               Hongsheng Li},\n",
        "  title     = {Efficient Attention: Attention with Linear Complexities},\n",
        "  journal   = {CoRR},  \n",
        "  year      = {2018},\n",
        "  url       = {http://arxiv.org/abs/1812.01243},\n",
        "}\n",
        "\n",
        "@misc{zhao2020image,\n",
        "    title  = {Image Augmentations for GAN Training},\n",
        "    author = {Zhengli Zhao and Zizhao Zhang and Ting Chen and Sameer Singh and Han Zhang},\n",
        "    year   = {2020},\n",
        "    eprint = {2006.02595},\n",
        "    archivePrefix = {arXiv}\n",
        "}\n",
        "\n",
        "@misc{karras2020training,\n",
        "    title   = {Training Generative Adversarial Networks with Limited Data},\n",
        "    author  = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},\n",
        "    year    = {2020},\n",
        "    eprint  = {2006.06676},\n",
        "    archivePrefix = {arXiv},\n",
        "    primaryClass = {cs.CV}\n",
        "}\n",
        "\n",
        "@article{article,\n",
        "author = {Krizhevsky, Alex},\n",
        "year = {2012},\n",
        "month = {05},\n",
        "pages = {},\n",
        "title = {Learning Multiple Layers of Features from Tiny Images},\n",
        "journal = {University of Toronto}\n",
        "}\n",
        "\n",
        "@misc{karras2020training,\n",
        "    title={Training Generative Adversarial Networks with Limited Data},\n",
        "    author={Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},\n",
        "    year={2020},\n",
        "    eprint={2006.06676},\n",
        "    archivePrefix={arXiv},\n",
        "    primaryClass={cs.CV}\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ]
    }
  ]
}