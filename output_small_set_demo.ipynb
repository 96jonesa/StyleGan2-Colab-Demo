{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "output_small_set_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1n7QhgLSaJVvFvDuN8Cui7VQsPbISYejt",
      "authorship_tag": "ABX9TyPsKAD2gG2FxVwYN4UHCVKo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/96jonesa/StyleGan2-Colab-Demo/blob/master/output_small_set_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kroOml2bv6aP",
        "colab_type": "text"
      },
      "source": [
        "# What is this?\n",
        "\n",
        "This is a demo to walk people through the results of training StyleGAN2 on various small datasets and under various configurations.\n",
        "\n",
        "You may have seen or be interested in the results generated from the highest resolution pretrained models at: https://thispersondoesnotexist.com/\n",
        "\n",
        "Explanations of the effects of certain critical techniques are provided. Most of these models were trained (at the time of writing this) for only about 6 hours on a single P100 GPU via Colab (typically with 4 models training on separate P100s simultaneously, for absolutely no cost). At the end, some results are shown from training for ~20 hours. Typically, models of this nature would be trained up to 100x as long. These will be trained further and this demo will be updated frequently in the near future. However, the results are already decent enough for the purposes of demonstration and comparison.\n",
        "\n",
        "All of the models were trained using a notebook I wrote to allow for easy training using the free resources provided by Colab and Google Drive, linked here:\n",
        "\n",
        "https://colab.research.google.com/drive/1prEbP9AgZnxGCXtZkP-pgqRJoHcHJPou?usp=sharing\n",
        "\n",
        "Using the PyTorch implementation of StyleGAN2 available at:\n",
        "\n",
        "https://github.com/lucidrains/stylegan2-pytorch\n",
        "\n",
        "The GitHub repo I made for this project is available at:\n",
        "\n",
        "https://github.com/96jonesa/StyleGan2-Colab-Demo\n",
        "\n",
        "The public directory on my Google Drive containing all sample images used in this notebook:\n",
        "\n",
        "https://drive.google.com/drive/folders/1gpZKmuvOnsuRmCo3MEcpST_WC1Laaz3W?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBnrSalUwC-B",
        "colab_type": "text"
      },
      "source": [
        "#How to use:\n",
        "\n",
        "Login to Google (Drive).\n",
        "\n",
        "Image displays are full size 8x8 grids of generated images\n",
        "\n",
        "You can either run all the cells then scroll through with Cmd/Ctrl+F9 or 'Runtime > Run all'\n",
        "\n",
        "Or you can step through cell-by-cell by running each cell individually with Cmd/Ctrl+Enter (runs current cell) or Shift+Enter (runs current cell and moves focus to next cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES2K7CXYwY8-",
        "colab_type": "text"
      },
      "source": [
        "# Some cool results to get you interested\n",
        "\n",
        "All of these images were generated by the trained models - none of these are real photos. Continue through the demo to see how various techniques work together to achieve this quality of generated image (starting from low quality results and progressively adding techniques and improving quality)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjI4SqAb10PU",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1JgSa97zNrpqwNi4KZWGjjPa2FXcsM9Rx\">\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1hlh99zuVo51zYeKbtmYlzDlXk7-ikjJG\">\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1mn2-rZYSuhIDhdOVBO3u4rrYSJZLySgg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVuwdtUVwcWM",
        "colab_type": "text"
      },
      "source": [
        "#Descriptions of datasets (citations at end of notebook):\n",
        "\n",
        "metfaces: \"image dataset of human faces extracted from works of art\" -NVlabs\n",
        "\n",
        "celeba: \"large-scale face attributes dataset with more than 200K celebrity images\" -Lie et al.\n",
        "\n",
        "afhq_dog: \"5,000 high-quality images [of dogs] at 512Ã—512 resolution\" -Choi et al.\n",
        "\n",
        "cifar10_horse: 5000 32x32 color images of horses -Krizhevsky"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYNaLHrrGDDL",
        "colab_type": "text"
      },
      "source": [
        "# How does StyleGAN2 work (somewhat technically)?\n",
        "\n",
        "GANs typically have a generator network and a discriminator network. The discriminator network is a glorified classification network - it seeks to determine if a given image came from the underlying data distribution under consideration (i.e. does it look like real data from the dataset being used for training). The generator network seeks to produce images that the discriminator will classify as having come from the underlying data distribution, however it does not get to look at the images in the dataset - it must improve based only on how the discriminator reacts to samples it produces. In order to generate diverse images, generator networks are given random noise as input. Typically during training, there discriminator network is fed some real data, then some generated data, then optimized via stochastic gradient descent. The generator network is optimized after getting the results back from the discriminator network, with fancy scheduling regarding how often each network is optimized (to enduce stabilization) depending on the GAN architecture.\n",
        "\n",
        "In StyleGAN2, the generator network is composed of a mapping network and a synthesis network. The mapping network is composed of a sequence of fully-connected layers which serve to transform the random noise input (latent code) into a vector in an intermediate latent space in which the factors of variation are more linear than in the original latent space (i.e. it maps the latent code into a space better topologically structured for the problem). Between the mapping and synthesis networks, affine transformations are learned to produce styles from the intermediate latent vector. These styles are fed as inputs to layers in modules of the synthesis network corresponding to different resolutions (from 4x4 up to the desired resolution). There are several convolutional (each along with standard deviation modulation and normalization) style blocks per resolution module. Per-style block scaling factors are also learned and applied to the noise before being input to intermdetiate layers of the corresponding style block. After a few style blocks, upsampling occurs to bump the network up to the next resolution (4x4 to 8x8 to 16x16 and so on) until the desired resolution is attained.\n",
        "\n",
        "StyleGAN2 also uses an exponential moving average of the generator (synthesis and mapping) network parameters to compensate for the oscillatory tendencies of GANs, as well as mixed regularities (occasionally using two latent space codes instead of one and simply switching from the intermediate latent vector produced by one to the intermediate latent vector produced by the other at a randomly selected point in the synthesis network, essentially mixing the styles which would be generated by the two). This difference in generated samples due to the addition of these techniques is demonstrated below.\n",
        "\n",
        "There has also been a very recent development in data augmentation that applies to GANs in general (differentiable augmentation), reducing the amount of data required to obtain high-quality results by (in some cases) orders of magnitude. The effect of applying this technique is demonstrated below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJLZThgv-iZQ",
        "colab_type": "text"
      },
      "source": [
        "# Download the generated images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4ribKtPmLVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilities for downloading publicly shared Google Drive files (from my Google Drive).\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = 'https://docs.google.com/uc?export=download'\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, 'wb') as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WK_hj5WP7yE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "from IPython.display import Image, display\n",
        "\n",
        "file_id = '1nm-yOTaibATgFlM_5rOsA4TB8wxRVAHg'\n",
        "destination = 'demo_samples.zip'\n",
        "download_file_from_google_drive(file_id, destination)\n",
        "zip_ref = zipfile.ZipFile('demo_samples.zip', 'r')\n",
        "zip_ref.extractall('demo_samples')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JStqPjwEwpkV",
        "colab_type": "text"
      },
      "source": [
        "# First things first\n",
        "\n",
        "First, we will look at sample images generated by the pretrained models using no differentiable augmentation and no attention. We expect the results to lack diversity (due to the lack of differentiable augmentation) and be of relatively poor quality (due to the limited training time, lack of differentiable augmentation, and lack of use of certain training techniques described further below). In order, these 8x8 image displays are from training on the following datasets:\n",
        "\n",
        "1. cifar10_horse  (32x32 resolution, 5000 images, 97000 iterations @ 4.52 it/s on P100 GPU)\n",
        "2. afhq_dog       (128x128 resolution, 5000 images, 38000 iterations @ 1.78 it/s on P100 GPU)\n",
        "3. metfaces       (128x128 resolution, 1336 images, 27000 iterations @ 1.27 it/s on P100 GPU)\n",
        "4. celeba         (128x128 resolution, 202599 images, 36000 iterations @ 1.64 it/s on P100 GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBH-kDLTy6jK",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, no differentiable augmentation, no attention, standard:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GE_mKFzy6bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_97.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABCpLTymy6VV",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, no differentiable augmentation, no attention, standard:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPthTWWQy6Pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_38.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svxSniB7y6JR",
        "colab_type": "text"
      },
      "source": [
        "metfaces, no differentiable augmentation, no attention, standard:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTNeiibFy6El",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_27.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgclBVWJy591",
        "colab_type": "text"
      },
      "source": [
        "celeba, no differentiable augmentation, no attention, standard:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd12dao-y50o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_36.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j8HbxbLw3PM",
        "colab_type": "text"
      },
      "source": [
        "# EMA parameters\n",
        "\n",
        "Notice the poor quality? The model also generates samples from a version of the model which use an exponential moving average of the generator (synthesis and mapping) network parameters to compensate for the oscillatory tendencies of GANs. This improves stability and quality significantly. The result is shown below for the same models as above:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mk9uXhHzpR6",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG8K3W1izpLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_97-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNw0J7zKzoCA",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L7FmMxPzn8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_38-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV725tPfzn1j",
        "colab_type": "text"
      },
      "source": [
        "metfaces, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX0z1JMPznvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_27-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvI8GNLDznpQ",
        "colab_type": "text"
      },
      "source": [
        "celeba, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdqzvqA_znhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_36-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8RP_euQxJ2-",
        "colab_type": "text"
      },
      "source": [
        "# Mixed regularities\n",
        "\n",
        "The model also generates samples using EMA generator (synthesis and mapping) parameters as well as mixed regularities (using two latent space codes instead of one and simply switching from the intermediate latent vector produced by one to the intermediate latent vector produced by the other at a randomly selected point in the synthesis network, essentially mixing the styles which would be generated by the two), which causes a decorrelation of neighboring styles and thus allows for more fine-tuned diversity (however, I have found that when under-trained this leads to lower perceptual quality, presumably because the model no longer over-focuses on each distinct style). The images are presented as interpolations between styles both from left to right and from top to bottom:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea7rhjfhz-bm",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGCMVnbTz-Tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_97-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUaR5DtHz-K1",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKiqsH-_z-E8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_38-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD1nbrbEz9_r",
        "colab_type": "text"
      },
      "source": [
        "metfaces, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaTlxHpgz96d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_27-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EYo7krsz90s",
        "colab_type": "text"
      },
      "source": [
        "celeba, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUEYl_cYz9vA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_36-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U83NAbbQxT3G",
        "colab_type": "text"
      },
      "source": [
        "# Early stopping\n",
        "\n",
        "By now you have likely noticed the extremely low quality of the afhq_dog model. This model has drifted too far from proper convergence due to repeated failures to convince the discriminator with samples that were moving in the right direction, so it has given in to pressures away from the correct direction and is now repeatedly outputting garbage. In a soon-to-come update to this demo, the use of lower learning rates will be explored as a means of avoiding this issue. Here are the EMA generator (synthesis and mapping) parameters and mixed regularities outputs from an earlier training checkpoint of the same model (22000 iterations instead of 38000 iterations):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIbnxTe60adS",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog early stop, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eClTlVfO0aWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_22-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDp7668e0aRN",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog early stop, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDQoQliM0aJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_22-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSmgsqMCveFx",
        "colab_type": "text"
      },
      "source": [
        "# Differentiable augmentation\n",
        "\n",
        "By now you have likely noticed the extreme lack of diversity in the afhq_dog and metfaces models, as well as a more subtle lack of diversity in the cifar10_horses and celeba models. This is due to the small amount of data available in the former two datasets, and a reasonable yet still lacking amount of data in the latter two datasets. A recent innovation in data augmentation which uses differentiable augmentations of the data has led to models obtaining high quality results with up to 70x less data. As long as the probability of applying these augmentations to each training sample is kept low enough, the generated images will not contain artifacts of the augmentations (e.g. warping, cropped removals, etc.). The following show the results of training the above models while applying this augmentation to the discriminator input with probability 0.2 at each iteration (the EMA generator (synthesis and mapping) parameters results are shown, without mixed regularities). In order, these were trained on:\n",
        "\n",
        "1. cifar10_horse  (32x32 resolution, 5000 images, 97000 iterations @ 4.51 it/s on P100 GPU)\n",
        "2. afhq_dog       (128x128 resolution, 5000 images, 38000 iterations @ 1.56 it/s on P100 GPU)\n",
        "3. metfaces       (128x128 resolution, 1336 images, 27000 iterations @ 1.05 it/s on P100 GPU)\n",
        "4. celeba         (128x128 resolution, 202599 images, 36000 iterations @ 1.64 it/s on P100 GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi3-99lm1UG3",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQDgvvvK1T_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_02_attn_none_97-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LeIgc0A1T2B",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta8pnA2O1TuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_02_attn_none_38-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uPHTjja1TSc",
        "colab_type": "text"
      },
      "source": [
        "metfaces, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhXFON1P1TKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_02_attn_none_27-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWKq6QqH1TDD",
        "colab_type": "text"
      },
      "source": [
        "celeba, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZF5nAug1S5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_02_attn_none_36-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHzZXNVNxaB1",
        "colab_type": "text"
      },
      "source": [
        "# Cool, but let's see that with mixed regularities\n",
        "\n",
        "The same augmented data models produced the following sample images using mixed regularities:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7DxGssi1uRZ",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ycQqp5y1uIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_02_attn_none_97-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN2vA_qC1t_f",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vqyt9Ov1t5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_02_attn_none_38-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC7iPrXW1txp",
        "colab_type": "text"
      },
      "source": [
        "metfaces, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2RVlTOY1toN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_02_attn_none_27-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcF3o8Ij1thg",
        "colab_type": "text"
      },
      "source": [
        "celeba, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBzM6Jya1tVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_02_attn_none_36-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RMQ6Q_JrSx7",
        "colab_type": "text"
      },
      "source": [
        "# Longer training\n",
        "\n",
        "All of the results thus far have been from models trained for only about 6 hours each - in order to obtain better results we must train longer. The following show the results of training the above models while applying differentiable augmentation to the discriminator input with probability 0.2 at each iteration (same as above), but this time trained more than three times as long. The EMA results are shown, and are of significantly better quality than the results from the models in their less trained states. In order, these were trained on:\n",
        "\n",
        "1. cifar10_horse (32x32 resolution, 5000 images, 314000 iterations @ 4.51 it/s on P100 GPU)\n",
        "\n",
        "2. afhq_dog (128x128 resolution, 5000 images, 119000 iterations @ 1.56 it/s on P100 GPU)\n",
        "\n",
        "3. metfaces (128x128 resolution, 1336 images, 88000 iterations @ 1.05 it/s on P100 GPU)\n",
        "\n",
        "4. celeba (128x128 resolution, 202599 images, 120000 iterations @ 1.64 it/s on P100 GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0VnP_ResJ3U",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pYGYqmWsIYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_02_attn_none_314-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUZRw62ssIJa",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJTWAm39sIEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_02_attn_none_119-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJXfyY_9sH_t",
        "colab_type": "text"
      },
      "source": [
        "metfaces, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNHlGLeqsH7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_02_attn_none_88-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2VfiZKIsH2q",
        "colab_type": "text"
      },
      "source": [
        "celeba, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xka3_BD4sJfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_02_attn_none_120-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwOhVRZfs-j0",
        "colab_type": "text"
      },
      "source": [
        "# And the mixed regularities results..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txxAS0OotGam",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p0PtUeotF7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_02_attn_none_314-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQdLNu9WtFka",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YqATUHFtFcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_02_attn_none_119-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ3R1EwjtFV4",
        "colab_type": "text"
      },
      "source": [
        "metfaces, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbuyc6mZtFKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_02_attn_none_88-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQucVQmtTki",
        "colab_type": "text"
      },
      "source": [
        "celeba, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trUY79K8tGzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba__aug_02_attn_none_120-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aOXh3A85m-XG"
      },
      "source": [
        "# Longer training without differentiable augmentation\n",
        "\n",
        "The following shows the results of continued training on the models without differentiable augmentation. The EMA results are shown, and although they have improved on the results from the less trained models without augmentation, they are much worse than the results from further training with augmentation. The models (except for celeba, which has a much larger dataset) have collapsed into single representations of poor quality, and the afhq_dog model failed to stabilize after several attempts at training. In the case of cifar10_horse, there appears to be a case of overtraining due to the smaller image size - expect an update soon demonstrating and explaining the deterioration of the quality from its peak. In order, these were trained on:\n",
        "\n",
        "1. cifar10_horse (32x32 resolution, 5000 images, 273000 iterations @ 4.51 it/s on P100 GPU)\n",
        "\n",
        "2. afhq_dog (128x128 resolution, 5000 images, 76000 iterations @ 1.56 it/s on P100 GPU)\n",
        "\n",
        "3. metfaces (128x128 resolution, 1336 images, 81000 iterations @ 1.05 it/s on P100 GPU)\n",
        "\n",
        "4. celeba (128x128 resolution, 202599 images, 112000 iterations @ 1.64 it/s on P100 GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1ToGhK88m-XH"
      },
      "source": [
        "cifar10_horse, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BQOdYzYQm-XH",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_273-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R-XNQOQFm-XL"
      },
      "source": [
        "afhq_dog, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j9_Jf9s6m-XL",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_76-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nF4hI5cxm-XN"
      },
      "source": [
        "metfaces, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LYGyxjW3m-XN",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_81-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YE2w2CR3m-XP"
      },
      "source": [
        "celeba, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RmWTNMJLm-XQ",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_112-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oCoB0L67m-XS"
      },
      "source": [
        "# And the mixed regularities results without augmentation..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t3lGpX26m-XS"
      },
      "source": [
        "cifar10_horse, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ThdFnR6bm-XT",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_273-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vjyxdjQOm-XV"
      },
      "source": [
        "afhq_dog, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1YF2Dc7Cm-XV",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_76-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WnLSoRLjm-XX"
      },
      "source": [
        "metfaces, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TajyZLOzm-XX",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_81-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LdYe3rcmm-XZ"
      },
      "source": [
        "celeba, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MXgnP-Yxm-XZ",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_112-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMXbxGB84K-S",
        "colab_type": "text"
      },
      "source": [
        "# Higher resolution (256x256)\n",
        "\n",
        "An obvious bound on the quality of images is the image resolution - so far we have only generated 128x128 images (and 32x32 in the case of cifar10_horse). However, the afhq_dog and metfaces datasets are 1024x1024, so we can train models on them to generate images up to that resolution. Here we present the results from training at 256x256 resolution with differentiable augmentation (first the EMA results) - notice how the images appear to be warped in some cases (especially in the metfaces model), a result of the differential augmentation bleeding through to the generator (this can and will be fixed by training for a longer time and/or decreasing the augmentation probability):\n",
        "\n",
        "1. afhq_dog (256x256 resolution, 5000 images, 72000 iterations @ 1.10 s/it on P100 GPU)\n",
        "\n",
        "2. metfaces (256x256 resolution, 1336 images, 68000 iterations @ 1.10 s/it on P100 GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU_OWf_xytFV",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog 256x256, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXKC6yQNys-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_size_256_aug_02_attn_none_72-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDxd_3LRys3E",
        "colab_type": "text"
      },
      "source": [
        "metfaces 256x256, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NTsFdwzysfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_size_256_aug_02_attn_none_68-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1_TL6Rn5CFM",
        "colab_type": "text"
      },
      "source": [
        "# And now the mixed regularities results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u9Q7dbZczec9"
      },
      "source": [
        "afhq_dog 256x256, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rGthcmmRzec-",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_size_256_aug_02_attn_none_72-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jQORWfkUzedB"
      },
      "source": [
        "metfaces 256x256, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u40uRSaGzedB",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_size_256_aug_02_attn_none_68-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBufbo4Ox570",
        "colab_type": "text"
      },
      "source": [
        "# Coming soon:\n",
        "\n",
        "Results from training with larger image sizes (higher resolutions - on the high-resolution datasets).\n",
        "\n",
        "Results from training using attention (absent from StyleGAN2) on every layer, with and without augmentation. (Initial efforts have shown poor quality and ubstable results)\n",
        "\n",
        "Results from using various lower learning rates to improve model stability and diversity of generated results.\n",
        "\n",
        "Results from training with various other differentiable augmentation probabilities (0.1 and 0.3).\n",
        "\n",
        "Results from training with contrastive loss (absent from StyleGAN2).\n",
        "\n",
        "Results from training on additional interesting small datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0GhmmYg4dyM",
        "colab_type": "text"
      },
      "source": [
        "# Citations:\n",
        "\n",
        "```\n",
        "@inproceedings{choi2020starganv2,\n",
        "  title={StarGAN v2: Diverse Image Synthesis for Multiple Domains},\n",
        "  author={Yunjey Choi and Youngjung Uh and Jaejun Yoo and Jung-Woo Ha},\n",
        "  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n",
        "  year={2020}\n",
        "}\n",
        "\n",
        "@inproceedings{liu2015faceattributes,\n",
        " title = {Deep Learning Face Attributes in the Wild},\n",
        " author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},\n",
        " booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},\n",
        " month = {December},\n",
        " year = {2015} \n",
        "}\n",
        "\n",
        "@article{Karras2019stylegan2,\n",
        "  title   = {Analyzing and Improving the Image Quality of {StyleGAN}},\n",
        "  author  = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},\n",
        "  journal = {CoRR},\n",
        "  volume  = {abs/1912.04958},\n",
        "  year    = {2019},\n",
        "}\n",
        "\n",
        "@misc{zhao2020feature,\n",
        "    title   = {Feature Quantization Improves GAN Training},\n",
        "    author  = {Yang Zhao and Chunyuan Li and Ping Yu and Jianfeng Gao and Changyou Chen},\n",
        "    year    = {2020}\n",
        "}\n",
        "\n",
        "@misc{chen2020simple,\n",
        "    title   = {A Simple Framework for Contrastive Learning of Visual Representations},\n",
        "    author  = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},\n",
        "    year    = {2020}\n",
        "}\n",
        "\n",
        "@article{,\n",
        "  title     = {Oxford 102 Flowers},\n",
        "  author    = {Nilsback, M-E. and Zisserman, A., 2008},\n",
        "  abstract  = {A 102 category dataset consisting of 102 flower categories, commonly occuring in the United Kingdom. Each class consists of 40 to 258 images. The images have large scale, pose and light variations.}\n",
        "}\n",
        "\n",
        "@article{afifi201911k,\n",
        "  title   = {11K Hands: gender recognition and biometric identification using a large dataset of hand images},\n",
        "  author  = {Afifi, Mahmoud},\n",
        "  journal = {Multimedia Tools and Applications}\n",
        "}\n",
        "\n",
        "@misc{zhang2018selfattention,\n",
        "    title   = {Self-Attention Generative Adversarial Networks},\n",
        "    author  = {Han Zhang and Ian Goodfellow and Dimitris Metaxas and Augustus Odena},\n",
        "    year    = {2018},\n",
        "    eprint  = {1805.08318},\n",
        "    archivePrefix = {arXiv}\n",
        "}\n",
        "\n",
        "@article{shen2019efficient,\n",
        "  author    = {Zhuoran Shen and\n",
        "               Mingyuan Zhang and\n",
        "               Haiyu Zhao and\n",
        "               Shuai Yi and\n",
        "               Hongsheng Li},\n",
        "  title     = {Efficient Attention: Attention with Linear Complexities},\n",
        "  journal   = {CoRR},  \n",
        "  year      = {2018},\n",
        "  url       = {http://arxiv.org/abs/1812.01243},\n",
        "}\n",
        "\n",
        "@misc{zhao2020image,\n",
        "    title  = {Image Augmentations for GAN Training},\n",
        "    author = {Zhengli Zhao and Zizhao Zhang and Ting Chen and Sameer Singh and Han Zhang},\n",
        "    year   = {2020},\n",
        "    eprint = {2006.02595},\n",
        "    archivePrefix = {arXiv}\n",
        "}\n",
        "\n",
        "@misc{karras2020training,\n",
        "    title   = {Training Generative Adversarial Networks with Limited Data},\n",
        "    author  = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},\n",
        "    year    = {2020},\n",
        "    eprint  = {2006.06676},\n",
        "    archivePrefix = {arXiv},\n",
        "    primaryClass = {cs.CV}\n",
        "}\n",
        "\n",
        "@article{article,\n",
        "author = {Krizhevsky, Alex},\n",
        "year = {2012},\n",
        "month = {05},\n",
        "pages = {},\n",
        "title = {Learning Multiple Layers of Features from Tiny Images},\n",
        "journal = {University of Toronto}\n",
        "}\n",
        "\n",
        "@misc{karras2020training,\n",
        "    title={Training Generative Adversarial Networks with Limited Data},\n",
        "    author={Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},\n",
        "    year={2020},\n",
        "    eprint={2006.06676},\n",
        "    archivePrefix={arXiv},\n",
        "    primaryClass={cs.CV}\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ]
    }
  ]
}