{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "output_small_set_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1n7QhgLSaJVvFvDuN8Cui7VQsPbISYejt",
      "authorship_tag": "ABX9TyNjOxryIGAtc/BkmR49TF0m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/96jonesa/StyleGan2-Colab-Demo/blob/master/output_small_set_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kroOml2bv6aP",
        "colab_type": "text"
      },
      "source": [
        "# What is this?\n",
        "\n",
        "This is a demo to walk people through the results of training StyleGAN2 on various small datasets and under various configurations. Explanations of the effects of certain critical techniques are provided. Most if not all of these models were trained (at the time of writing this) for only about 6 hours on a single P100 GPU via Colab (typically with 4 models training on separate P100s simultaneously). Typically, models of this nature would be trained up to 100x as long. These will be trained further and this demo will be updated frequently in the near future. However, the results are already decent enough for the purposes of demonstration and comparison.\n",
        "\n",
        "All of the models were trained using a notebook I wrote to allow for easy training using the free resources provided by Colab and Google Drive, linked here:\n",
        "\n",
        "https://colab.research.google.com/drive/1prEbP9AgZnxGCXtZkP-pgqRJoHcHJPou?usp=sharing\n",
        "\n",
        "Using the PyTorch implementation of StyleGAN2 available at:\n",
        "\n",
        "https://github.com/lucidrains/stylegan2-pytorch\n",
        "\n",
        "The GitHub repo I made for this project is available at:\n",
        "\n",
        "https://github.com/96jonesa/StyleGan2-Colab-Demo\n",
        "\n",
        "The public directory on my Google Drive containing all sample images used in this notebook:\n",
        "\n",
        "https://drive.google.com/drive/folders/1gpZKmuvOnsuRmCo3MEcpST_WC1Laaz3W?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBnrSalUwC-B",
        "colab_type": "text"
      },
      "source": [
        "#How to use:\n",
        "\n",
        "Login to Google (Drive).\n",
        "\n",
        "Image displays are full size 8x8 grids of 128x128 images (32x32 in case of cifar10). You will need to scroll through the cell output to see each of the (typically 4) grids displayed in each output. Simply place you cursor somewhere inside the output, then scroll.\n",
        "\n",
        "You can either run all the cells then scroll through with Cmd/Ctrl+F9 or 'Runtime > Run all'\n",
        "\n",
        "Or you can step through cell-by-cell by running each cell individually with Cmd/Ctrl+Enter (runs current cell) or Shift+Enter (runs current cell and moves focus to next cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVuwdtUVwcWM",
        "colab_type": "text"
      },
      "source": [
        "#Descriptions of datasets (citations at end of notebook):\n",
        "\n",
        "metfaces: \"image dataset of human faces extracted from works of art\" -NVlabs\n",
        "\n",
        "celeba: \"large-scale face attributes dataset with more than 200K celebrity images\" -Lie et al.\n",
        "\n",
        "afhq_dog: \"5,000 high-quality images [of dogs] at 512Ã—512 resolution\" -Choi et al.\n",
        "\n",
        "cifar10_horse: 5000 32x32 color images of horses -Krizhevsky"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYNaLHrrGDDL",
        "colab_type": "text"
      },
      "source": [
        "# How does StyleGAN2 work (somewhat technically)?\n",
        "\n",
        "GANs typically have a generator network and a discriminator network. The discriminator network is a glorified classification network - it seeks to determine if a given image came from the underlying data distribution under consideration (i.e. does it look like real data from the dataset being used for training). The generator network seeks to produce images that the discriminator will classify as having come from the underlying data distribution, however it does not get to look at the images in the dataset - it must improve based only on how the discriminator reacts to samples it produces. In order to generate diverse images, generator networks are given random noise as input. Typically during training, there discriminator network is fed some real data, then some generated data, then optimized via stochastic gradient descent. The generator network is optimized after getting the results back from the discriminator network, with fancy scheduling regarding how often each network is optimized (to enduce stabilization) depending on the GAN architecture.\n",
        "\n",
        "In StyleGAN2, the generator network is composed of a mapping network and a synthesis network. The mapping network is composed of a sequence of fully-connected layers which serve to transform the random noise input (latent code) into a vector in an intermediate latent space in which the factors of variation are more linear than in the original latent space (i.e. it maps the latent code into a space better topologically structured for the problem). Between the mapping and synthesis networks, affine transformations are learned to produce styles from the intermediate latent vector. These styles are fed as inputs to layers in modules of the synthesis network corresponding to different resolutions (from 4x4 up to the desired resolution). There are several convolutional (each along with standard deviation modulation and normalization) style blocks per resolution module. Per-style block scaling factors are also learned and applied to the noise before being input to intermdetiate layers of the corresponding style block. After a few style blocks, upsampling occurs to bump the network up to the next resolution (4x4 to 8x8 to 16x16 and so on) until the desired resolution is attained.\n",
        "\n",
        "StyleGAN2 also uses an exponential moving average of the generator (synthesis and mapping) network parameters to compensate for the oscillatory tendencies of GANs, as well as mixed regularities (occasionally using two latent space codes instead of one and simply switching from the intermediate latent vector produced by one to the intermediate latent vector produced by the other at a randomly selected point in the synthesis network, essentially mixing the styles which would be generated by the two). This difference in generated samples due to the addition of these techniques is demonstrated below.\n",
        "\n",
        "There has also been a very recent development in data augmentation that applies to GANs in general (differentiable augmentation), reducing the amount of data required to obtain high-quality results by (in some cases) orders of magnitude. The effect of applying this technique is demonstrated below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJLZThgv-iZQ",
        "colab_type": "text"
      },
      "source": [
        "# Download the sample images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4ribKtPmLVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilities for downloading publicly shared Google Drive files (from my Google Drive).\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = 'https://docs.google.com/uc?export=download'\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, 'wb') as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WK_hj5WP7yE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "from IPython.display import Image, display\n",
        "\n",
        "file_id = '1uEuLbFWRdAbaB2p3pvQOUUMj7_FJF3bN'\n",
        "destination = 'demo_samples.zip'\n",
        "download_file_from_google_drive(file_id, destination)\n",
        "zip_ref = zipfile.ZipFile('demo_samples.zip', 'r')\n",
        "zip_ref.extractall('demo_samples')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JStqPjwEwpkV",
        "colab_type": "text"
      },
      "source": [
        "# First things first\n",
        "\n",
        "First, we will look at sample images generated by the pretrained models using no differentiable augmentation and no attention. We expect the results to lack diversity (due to the lack of differentiable augmentation) and be of relatively poor quality (due to the lack of attention). In order, these 8x8 image displays are from training on the following datasets:\n",
        "\n",
        "1. cifar10_horse  (32x32 resolution, 5000 images, 97000 iterations @ 4.52 it/s on P100 GPU)\n",
        "2. afhq_dog       (128x128 resolution, 5000 images, 38000 iterations @ 1.78 it/s on P100 GPU)\n",
        "3. metfaces       (128x128 resolution, 1336 images, 27000 iterations @ 1.27 it/s on P100 GPU)\n",
        "4. celeba         (128x128 resolution, 202599 images, 36000 iterations @ 1.64 it/s on P100 GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBH-kDLTy6jK",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, no differentiable augmentation, no attention, standard:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GE_mKFzy6bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_97.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABCpLTymy6VV",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, no differentiable augmentation, no attention, standard:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPthTWWQy6Pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_38.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svxSniB7y6JR",
        "colab_type": "text"
      },
      "source": [
        "metfaces, no differentiable augmentation, no attention, standard:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTNeiibFy6El",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_27.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgclBVWJy591",
        "colab_type": "text"
      },
      "source": [
        "celeba, no differentiable augmentation, no attention, standard:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd12dao-y50o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_36.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j8HbxbLw3PM",
        "colab_type": "text"
      },
      "source": [
        "# EMA parameters\n",
        "\n",
        "Notice the poor quality? The model also generates samples from a version of the model which use an exponential moving average of the generator (synthesis and mapping) network parameters to compensate for the oscillatory tendencies of GANs. This improves stability and quality significantly. The result is shown below for the same models as above:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mk9uXhHzpR6",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG8K3W1izpLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_97-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNw0J7zKzoCA",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L7FmMxPzn8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_38-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV725tPfzn1j",
        "colab_type": "text"
      },
      "source": [
        "metfaces, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX0z1JMPznvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_27-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvI8GNLDznpQ",
        "colab_type": "text"
      },
      "source": [
        "celeba, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdqzvqA_znhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_36-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8RP_euQxJ2-",
        "colab_type": "text"
      },
      "source": [
        "# Mixed regularities\n",
        "\n",
        "The model also generates samples using EMA generator (synthesis and mapping) parameters as well as mixed regularities (using two latent space codes instead of one and simply switching from the intermediate latent vector produced by one to the intermediate latent vector produced by the other at a randomly selected point in the synthesis network, essentially mixing the styles which would be generated by the two), which causes a decorrelation of neighboring styles and thus allows for more fine-tuned diversity (however, I have found that when under-trained this leads to lower perceptual quality, presumably because the model no longer over-focuses on each distinct style):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea7rhjfhz-bm",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGCMVnbTz-Tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_00_attn_none_97-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUaR5DtHz-K1",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKiqsH-_z-E8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_38-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD1nbrbEz9_r",
        "colab_type": "text"
      },
      "source": [
        "metfaces, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaTlxHpgz96d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_00_attn_none_27-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EYo7krsz90s",
        "colab_type": "text"
      },
      "source": [
        "celeba, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUEYl_cYz9vA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_00_attn_none_36-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U83NAbbQxT3G",
        "colab_type": "text"
      },
      "source": [
        "# Early stopping\n",
        "\n",
        "By now you have likely noticed the extremely low quality of the afhq_dog model. This model has drifted too far from proper convergence due to repeated failures to convince the discriminator with samples that were moving in the right direction, so it has given in to pressures away from the correct direction and is now repeatedly outputting garbage. In a soon-to-come update to this demo, the use of lower learning rates will be explored as a means of avoiding this issue. Here are the EMA generator (synthesis and mapping) parameters and mixed regularities outputs from an earlier training checkpoint of the same model (22000 iterations instead of 38000 iterations):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIbnxTe60adS",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse early stop, no differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eClTlVfO0aWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_22-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDp7668e0aRN",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse early stop, no differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDQoQliM0aJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_00_attn_none_22-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSmgsqMCveFx",
        "colab_type": "text"
      },
      "source": [
        "# Differentiable augmentation\n",
        "\n",
        "By now you have likely noticed the extreme lack of diversity in the afhq_dog and metfaces models, as well as a more subtle lack of diversity in the cifar10_horses and celeba models. This is due to the small amount of data available in the former two datasets, and a reasonable yet still lacking amount of data in the latter two datasets. A recent innovation in data augmentation which uses differentiable augmentations of the data has led to models obtaining high quality results with up to 70x less data. The following show the results of training the above models while applying this augmentation to the discriminator input with probability 0.2 at each iteration (the EMA generator (synthesis and mapping) parameters results are shown, without mixed regularities). In order, these were trained on:\n",
        "\n",
        "1. cifar10_horse  (32x32 resolution, 5000 images, 97000 iterations @ 4.51 it/s on P100 GPU)\n",
        "2. afhq_dog       (128x128 resolution, 5000 images, 38000 iterations @ 1.56 it/s on P100 GPU)\n",
        "3. metfaces       (128x128 resolution, 1336 images, 27000 iterations @ 1.05 it/s on P100 GPU)\n",
        "4. celeba         (128x128 resolution, 202599 images, 36000 iterations @ 1.64 it/s on P100 GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi3-99lm1UG3",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQDgvvvK1T_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_02_attn_none_97-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LeIgc0A1T2B",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta8pnA2O1TuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_02_attn_none_38-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uPHTjja1TSc",
        "colab_type": "text"
      },
      "source": [
        "metfaces, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhXFON1P1TKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_02_attn_none_27-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWKq6QqH1TDD",
        "colab_type": "text"
      },
      "source": [
        "celeba, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZF5nAug1S5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_02_attn_none_36-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHzZXNVNxaB1",
        "colab_type": "text"
      },
      "source": [
        "# Cool, but let's see that with mixed regularities\n",
        "\n",
        "The same augmented data models produced the following sample images using mixed regularities:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7DxGssi1uRZ",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ycQqp5y1uIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_02_attn_none_97-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN2vA_qC1t_f",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vqyt9Ov1t5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_02_attn_none_38-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC7iPrXW1txp",
        "colab_type": "text"
      },
      "source": [
        "metfaces, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2RVlTOY1toN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_02_attn_none_27-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcF3o8Ij1thg",
        "colab_type": "text"
      },
      "source": [
        "celeba, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBzM6Jya1tVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_02_attn_none_36-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RMQ6Q_JrSx7",
        "colab_type": "text"
      },
      "source": [
        "# Longer training\n",
        "\n",
        "The following show the results of training the above models while applying differentiable augmentation to the discriminator input with probability 0.2 at each iteration (same as above), but this time trained over three times as long. The EMA results are shows. In order, these were trained on:\n",
        "\n",
        "1. cifar10_horse (32x32 resolution, 5000 images, 314000 iterations @ 4.51 it/s on P100 GPU)\n",
        "\n",
        "2. afhq_dog (128x128 resolution, 5000 images, 119000 iterations @ 1.56 it/s on P100 GPU)\n",
        "\n",
        "3. metfaces (128x128 resolution, 1336 images, 88000 iterations @ 1.05 it/s on P100 GPU)\n",
        "\n",
        "4. celeba (128x128 resolution, 202599 images, 120000 iterations @ 1.64 it/s on P100 GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0VnP_ResJ3U",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pYGYqmWsIYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_02_attn_none_314-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUZRw62ssIJa",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJTWAm39sIEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_02_attn_none_119-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJXfyY_9sH_t",
        "colab_type": "text"
      },
      "source": [
        "metfaces, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNHlGLeqsH7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_02_attn_none_88-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2VfiZKIsH2q",
        "colab_type": "text"
      },
      "source": [
        "celeba, 0.2 differentiable augmentation, no attention, EMA parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xka3_BD4sJfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_02_attn_none_120-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwOhVRZfs-j0",
        "colab_type": "text"
      },
      "source": [
        "# And the mixed regularities results..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txxAS0OotGam",
        "colab_type": "text"
      },
      "source": [
        "cifar10_horse, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p0PtUeotF7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/cifar10_horse_aug_02_attn_none_314-mr.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQdLNu9WtFka",
        "colab_type": "text"
      },
      "source": [
        "afhq_dog, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YqATUHFtFcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/afhq_dog_aug_02_attn_none_119-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ3R1EwjtFV4",
        "colab_type": "text"
      },
      "source": [
        "metfaces, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbuyc6mZtFKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/metfaces_aug_02_attn_none_88-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQucVQmtTki",
        "colab_type": "text"
      },
      "source": [
        "celeba, 0.2 differentiable augmentation, no attention, mixed regularities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trUY79K8tGzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('/content/demo_samples/StyleGan2_small_set_demo_samples/celeba_aug_02_attn_none_120-ema.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBufbo4Ox570",
        "colab_type": "text"
      },
      "source": [
        "# Coming soon:\n",
        "\n",
        "Results from training using attention (absent from StyleGAN2) on every layer, with and without augmentation.\n",
        "\n",
        "Results from using various lower learning rates to improve model stability and diversity of generated results.\n",
        "\n",
        "Results from training with various other differentiable augmentation probabilities (0.1 and 0.3).\n",
        "\n",
        "Results from training with contrastive loss (absent from StyleGAN2).\n",
        "\n",
        "Results from training for more iterations under these configurations.\n",
        "\n",
        "Results from training with larger image sizes (higher resolutions - on the high-resolution datasets).\n",
        "\n",
        "Results from training on additional interesting small datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0GhmmYg4dyM",
        "colab_type": "text"
      },
      "source": [
        "# Citations:\n",
        "\n",
        "```\n",
        "@inproceedings{choi2020starganv2,\n",
        "  title={StarGAN v2: Diverse Image Synthesis for Multiple Domains},\n",
        "  author={Yunjey Choi and Youngjung Uh and Jaejun Yoo and Jung-Woo Ha},\n",
        "  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n",
        "  year={2020}\n",
        "}\n",
        "\n",
        "@inproceedings{liu2015faceattributes,\n",
        " title = {Deep Learning Face Attributes in the Wild},\n",
        " author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},\n",
        " booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},\n",
        " month = {December},\n",
        " year = {2015} \n",
        "}\n",
        "\n",
        "@article{Karras2019stylegan2,\n",
        "  title   = {Analyzing and Improving the Image Quality of {StyleGAN}},\n",
        "  author  = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},\n",
        "  journal = {CoRR},\n",
        "  volume  = {abs/1912.04958},\n",
        "  year    = {2019},\n",
        "}\n",
        "\n",
        "@misc{zhao2020feature,\n",
        "    title   = {Feature Quantization Improves GAN Training},\n",
        "    author  = {Yang Zhao and Chunyuan Li and Ping Yu and Jianfeng Gao and Changyou Chen},\n",
        "    year    = {2020}\n",
        "}\n",
        "\n",
        "@misc{chen2020simple,\n",
        "    title   = {A Simple Framework for Contrastive Learning of Visual Representations},\n",
        "    author  = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},\n",
        "    year    = {2020}\n",
        "}\n",
        "\n",
        "@article{,\n",
        "  title     = {Oxford 102 Flowers},\n",
        "  author    = {Nilsback, M-E. and Zisserman, A., 2008},\n",
        "  abstract  = {A 102 category dataset consisting of 102 flower categories, commonly occuring in the United Kingdom. Each class consists of 40 to 258 images. The images have large scale, pose and light variations.}\n",
        "}\n",
        "\n",
        "@article{afifi201911k,\n",
        "  title   = {11K Hands: gender recognition and biometric identification using a large dataset of hand images},\n",
        "  author  = {Afifi, Mahmoud},\n",
        "  journal = {Multimedia Tools and Applications}\n",
        "}\n",
        "\n",
        "@misc{zhang2018selfattention,\n",
        "    title   = {Self-Attention Generative Adversarial Networks},\n",
        "    author  = {Han Zhang and Ian Goodfellow and Dimitris Metaxas and Augustus Odena},\n",
        "    year    = {2018},\n",
        "    eprint  = {1805.08318},\n",
        "    archivePrefix = {arXiv}\n",
        "}\n",
        "\n",
        "@article{shen2019efficient,\n",
        "  author    = {Zhuoran Shen and\n",
        "               Mingyuan Zhang and\n",
        "               Haiyu Zhao and\n",
        "               Shuai Yi and\n",
        "               Hongsheng Li},\n",
        "  title     = {Efficient Attention: Attention with Linear Complexities},\n",
        "  journal   = {CoRR},  \n",
        "  year      = {2018},\n",
        "  url       = {http://arxiv.org/abs/1812.01243},\n",
        "}\n",
        "\n",
        "@misc{zhao2020image,\n",
        "    title  = {Image Augmentations for GAN Training},\n",
        "    author = {Zhengli Zhao and Zizhao Zhang and Ting Chen and Sameer Singh and Han Zhang},\n",
        "    year   = {2020},\n",
        "    eprint = {2006.02595},\n",
        "    archivePrefix = {arXiv}\n",
        "}\n",
        "\n",
        "@misc{karras2020training,\n",
        "    title   = {Training Generative Adversarial Networks with Limited Data},\n",
        "    author  = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},\n",
        "    year    = {2020},\n",
        "    eprint  = {2006.06676},\n",
        "    archivePrefix = {arXiv},\n",
        "    primaryClass = {cs.CV}\n",
        "}\n",
        "\n",
        "@article{article,\n",
        "author = {Krizhevsky, Alex},\n",
        "year = {2012},\n",
        "month = {05},\n",
        "pages = {},\n",
        "title = {Learning Multiple Layers of Features from Tiny Images},\n",
        "journal = {University of Toronto}\n",
        "}\n",
        "\n",
        "@misc{karras2020training,\n",
        "    title={Training Generative Adversarial Networks with Limited Data},\n",
        "    author={Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},\n",
        "    year={2020},\n",
        "    eprint={2006.06676},\n",
        "    archivePrefix={arXiv},\n",
        "    primaryClass={cs.CV}\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ]
    }
  ]
}